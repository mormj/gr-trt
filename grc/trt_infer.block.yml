id: trt_infer
label: Infer (TensorRT)
category: '[TensorRT]'
templates:
  imports: from gnuradio import trt
  make: trt.infer(${onnx_pathname}, ${type.size}, ${workspace_size}, ${dla_core})

parameters:
- id: type
  label: IO Type
  dtype: enum
  options: [complex, float, int, short, byte]
  option_attributes:
    size: [gr.sizeof_float, gr.sizeof_float, gr.sizeof_int, gr.sizeof_short,
        gr.sizeof_char]
  hide: part
  default: float
- id: onnx_pathname
  label: ONNX Model
  dtype: file_open
- id: workspace_size
  label: Workspace Size
  default: 1073741824
  dtype: int
- id: dla_core
  label: DLA Core
  default: -1
  dtype: int
inputs:
- label: in
  domain: cuda
  dtype: ${type}
  
outputs:
- label: out
  domain: cuda
  dtype: ${type}

file_format: 1
