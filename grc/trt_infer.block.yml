id: trt_infer
label: Infer (TensorRT)
category: '[TensorRT]'
templates:
  imports: import trt
  make: trt.infer(${onnx_pathname}, ${type.size}, ${memory_model}, ${workspace_size}, ${dla_core})

parameters:
- id: type
  label: IO Type
  dtype: enum
  options: [complex, float, int, short, byte]
  option_attributes:
    size: [gr.sizeof_float, gr.sizeof_float, gr.sizeof_int, gr.sizeof_short,
        gr.sizeof_char]
  hide: part
  default: float
- id: onnx_pathname
  label: ONNX Model
  dtype: file_open
- id: memory_model
  label: Memory Model
  dtype: enum
  default: trt.memory_model_t.TRADITIONAL
  options: [trt.memory_model_t.TRADITIONAL, trt.memory_model_t.PINNED, trt.memory_model_t.UNIFIED]
  option_labels: ['Traditional', 'Pinned', 'Unified']
- id: workspace_size
  label: Workspace Size
  default: 1073741824
  dtype: int
- id: dla_core
  label: DLA Core
  default: -1
  dtype: int
inputs:
- label: in
  domain: stream
  dtype: ${type}
outputs:
- label: out
  domain: stream
  dtype: ${type}

file_format: 1
