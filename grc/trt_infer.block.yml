id: trt_infer
label: infer
category: '[TensorRT]'
templates:
  imports: import trt
  make: trt.infer(${onnx_pathname}, ${type.size}, ${batch_size}, ${workspace_size}, ${dla_core})

parameters:
- id: type
  label: IO Type
  dtype: enum
  options: [complex, float, int, short, byte]
  option_attributes:
    size: [gr.sizeof_float, gr.sizeof_float, gr.sizeof_int, gr.sizeof_short,
        gr.sizeof_char]
  hide: part
  default: float
- id: onnx_pathname
  label: ONNX Model
  dtype: file_open
- id: batch_size
  label: Batch Size
  default: 1
  dtype: int
- id: workspace_size
  label: Workspace Size
  default: 1073741824
  dtype: int
- id: dla_core
  label: DLA Core
  default: -1
  dtype: int
inputs:
- label: in
  domain: stream
  dtype: ${type}
outputs:
- label: out
  domain: stream
  dtype: ${type}

file_format: 1
